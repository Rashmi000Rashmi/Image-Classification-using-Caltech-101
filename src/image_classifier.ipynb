{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f310d5fd-d7b5-4a69-af2b-65a386400c67",
   "metadata": {},
   "source": [
    "# Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedfdb96-2b99-4f15-a078-f8fa6c1661c5",
   "metadata": {},
   "source": [
    "### Import packages and libraries required for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbbdb230-5714-4005-899d-53873352c6fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 22:19:28.828770: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-18 22:19:28.828833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-18 22:19:28.988782: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-18 22:19:31.303417: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c6a227-cd4a-424b-a382-4d3fb5b40c08",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Upload and Load the caltech-101 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "257b5134-7522-4650-841a-9ba8d66f8aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "caltech101_path = '../data/caltech-101/101_ObjectCategories'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c4a7dd-88b4-436a-a4eb-53cbfc75c07c",
   "metadata": {},
   "source": [
    "### Display all the caltech-101 directories in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bda579fd-1fb0-4023-bdc3-f681c9e4ac21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BACKGROUND_Google', 'Faces', 'Faces_easy', 'Leopards', 'Motorbikes', 'accordion', 'airplanes', 'anchor', 'ant', 'barrel', 'bass', 'beaver', 'binocular', 'bonsai', 'brain', 'brontosaurus', 'buddha', 'butterfly', 'camera', 'cannon', 'car_side', 'ceiling_fan', 'cellphone', 'chair', 'chandelier', 'cougar_face', 'crab', 'crayfish', 'crocodile', 'crocodile_head', 'cup', 'dalmatian', 'dollar_bill', 'dolphin', 'dragonfly', 'electric_guitar', 'elephant', 'emu', 'euphonium', 'ewer', 'ferry', 'flamingo', 'flamingo_head', 'garfield', 'gerenuk', 'gramophone', 'grand_piano', 'hawksbill', 'headphone', 'hedgehog', 'helicopter', 'ibis', 'inline_skate', 'joshua_tree', 'kangaroo', 'ketch', 'lamp', 'laptop', 'llama', 'lobster', 'lotus', 'mandolin', 'mayfly', 'menorah', 'metronome', 'minaret', 'nautilus', 'octopus', 'okapi', 'pagoda', 'panda', 'pigeon', 'pizza', 'platypus', 'pyramid', 'revolver', 'rhino', 'rooster', 'saxophone', 'schooner', 'scissors', 'scorpion', 'sea_horse', 'snoopy', 'soccer_ball', 'stapler', 'starfish', 'stegosaurus', 'stop_sign', 'strawberry', 'sunflower', 'tick', 'trilobite', 'umbrella', 'watch', 'water_lilly', 'wheelchair', 'wild_cat', 'windsor_chair', 'wrench', 'yin_yang', 'cougar_body']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(caltech101_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e23d603-3f0e-4ba2-b1a0-aff45bf3fb47",
   "metadata": {},
   "source": [
    "### Create list image_data and labels to store the image paths and image labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3682f1a3-ff7d-4a52-9b32-104d9ee11649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3b46ac-82b6-4069-9d0a-e118db70f7dc",
   "metadata": {},
   "source": [
    "### Loop through the list of all the directories and store the image paths in image_data and image labels in labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1113896c-60dd-4080-b4e8-844d0a8e8531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for class_name in os.listdir(caltech101_path):\n",
    "    class_path = os.path.join(caltech101_path, class_name)\n",
    "\n",
    "    if os.path.isdir(class_path):\n",
    "        for image_file in os.listdir(class_path):\n",
    "            if image_file.endswith('.jpg'):\n",
    "                image_path = os.path.join(class_path, image_file)\n",
    "                image_data.append(image_path)\n",
    "                labels.append(class_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e81edaa-909d-4baf-b350-3277d09aa95f",
   "metadata": {},
   "source": [
    "## **Split train and test sets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e26b12d-fe82-4638-b26f-05384c1d4694",
   "metadata": {},
   "source": [
    "### Split the train and test sets randomly using train_test_split function with parameters image_data, labels, test_size=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ba69005-2d5e-4049-a4cc-543210985718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    image_data, labels, test_size=0.2, random_state=42, stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d51af6c-aabc-4212-baa2-61c069467152",
   "metadata": {},
   "source": [
    "### Create objects directory to store the train and test directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91a06ffb-c7e3-40b4-9a26-69454fd3aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_root = '../data/caltech-101/objects'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e1e7ce-56ab-4c89-a4bd-6566fa9b4fcb",
   "metadata": {},
   "source": [
    "### Create the path for train and test directories with the names train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89f8de1-5d1a-4130-9cda-98be26adaf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(train_test_root, 'train')\n",
    "test_dir = os.path.join(train_test_root, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b651171-20a9-4676-9a50-5ae33916f602",
   "metadata": {},
   "source": [
    "### Check train and test directories are existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde3dee9-174a-41f8-b8f0-5db1e60ff8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a29a40-8c79-45ee-88fb-03871c39f2b1",
   "metadata": {},
   "source": [
    "### Define a function copy_images to copy the images of train data to train directory and test data to test directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b05e108-149d-4b14-9988-183f178d1ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_images(src_paths, dest_dir):\n",
    "    for src_path in src_paths:\n",
    "        class_name = os.path.basename(os.path.dirname(src_path))\n",
    "        dest_class_dir = os.path.join(dest_dir, class_name)\n",
    "        os.makedirs(dest_class_dir, exist_ok=True)\n",
    "        \n",
    "        dest_path = os.path.join(dest_class_dir, os.path.basename(src_path))\n",
    "        \n",
    "        with open(src_path, 'rb') as src_file:\n",
    "            with open(dest_path, 'wb') as dest_file:\n",
    "                dest_file.write(src_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd782d0-5ad3-4c16-9f2c-7c261dc5f33f",
   "metadata": {},
   "source": [
    "### Call the function copy_images with arguments train_data which consists train images and path of the train directory(train_dir) to store the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e2af2d-adb7-423f-98c2-2c9d6968c55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_images(train_data, train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50456039-682f-4dba-8da2-c0e5ff09b334",
   "metadata": {},
   "source": [
    "### Call the function copy_images with arguments test_data which consists test images and path of the test directory(test_dir) to store the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c0428c-46b6-4b6d-bb50-bdfa2b9ba761",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "copy_images(test_data, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b1682e-9756-4479-a686-b70ee2621eca",
   "metadata": {},
   "source": [
    "## **Data augmentation** \n",
    "\n",
    "It involves applying various transformations to the existing dataset to artificially increase its size, diversity, and improve the model's generalization. Augmentation techniques include rotation, flipping, scaling, translation, shearing, and changes in brightness and contrast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f53ed3f-48dd-4278-9d99-4b4f31fef2fe",
   "metadata": {},
   "source": [
    "### Apply Data Augmentation for the training set using ImageDataGenerator with parameters rescale, shear_range, zoom_range, horizontal_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e67f72-b5a3-4998-bdf0-021b17c4a406",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5895403d-64a1-4967-af3b-e37c5caa5f60",
   "metadata": {},
   "source": [
    "### Configure ImageDataGenerator for normalizing test data by rescaling pixel values to the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c8f2116749fa62",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3849154f-3806-4c2e-bc11-fba1498c0e81",
   "metadata": {},
   "source": [
    "### Configure batch size of 32 for data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307e2e97e2347a2b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001edab-ccc2-44a8-88fa-9e96ade576ab",
   "metadata": {},
   "source": [
    "### Use flow_from_directory to generate batches of augmented and normalized images from the train directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff549fed17a51cb2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),  \n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef78613-c667-4fe2-a753-6ca3315a98af",
   "metadata": {},
   "source": [
    "### Use flow_from_directory to generate batches of normalized images from the test directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f1ead27d63ce6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d399ab99-3a9f-41bc-9a87-b4ddcdd20785",
   "metadata": {},
   "source": [
    "## **VGG16 Architecture**\n",
    "\n",
    "VGG16 architecture is characterized by a deliberate design choice, emphasizing simplicity with a focus on 3x3 convolutional layers using a stride of 1 and always employing same padding. The consistent pattern of convolution layers followed by 2x2 max-pooling layers with a stride of 2 is maintained throughout the entire structure. The term \"16\" in VGG16 denotes the presence of 16 layers with trainable weights. The network concludes with two fully connected layers and a softmax activation for output. VGG16 is a substantial model, boasting around 138 million parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa99ea10-055c-47b2-a936-fca06db4d5a7",
   "metadata": {},
   "source": [
    "### Load VGG16 as the vgg16_model with ImageNet weights, excluding top layers, and set input shape to (224, 224, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d9924a7ed32f7a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "vgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809acaa1-0cdb-48d1-a64d-bccfbc00c020",
   "metadata": {},
   "source": [
    "### Freeze all the layers of the vgg16_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393441ca79dc1d98",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for layer in vgg16_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6f7e18-fddc-4309-8641-f96726c3692d",
   "metadata": {},
   "source": [
    "### Built a sequential model on top of VGG16 with a flatten layer, a dense layer, a dropout layer , and a final dense layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ece1c2a38f8cdb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(vgg16_model)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(102, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10254502-6210-405a-a232-0b25ea8e071d",
   "metadata": {},
   "source": [
    "### Compile the model using the Adam optimizer, categorical cross-entropy loss function, and accuracy as the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda0a5461a744f77",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2c2b44-7862-4f9d-9d60-4c06de8185b7",
   "metadata": {},
   "source": [
    "### Train the model using model.fit with training generator, test generator, and specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1825f0-d646-40d6-af3c-733df6e9ae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=30,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples // batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec8d62b-86b9-429d-9b98-968e1ab5a22d",
   "metadata": {},
   "source": [
    "### Evaluate the model on test generator to obtain test loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d92544-f40c-47ad-9368-2d573f2220f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c5748-84e6-4c6b-97ba-2fe7cc36100b",
   "metadata": {},
   "source": [
    "## **Testing Model with the image input**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d009620a6736ce",
   "metadata": {},
   "source": [
    "### Define the path of the test image to the variable test_image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d8203a5e2fcee8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_image_path = '../data/test_image_1.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c970fb9d6ceeedb",
   "metadata": {},
   "source": [
    "### Load the image in the test_image_path and resize it to the target size that is (224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa5cdc5a58ecc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_img = image.load_img(test_image_path, target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d78a1abd8aea2b",
   "metadata": {},
   "source": [
    "### Convert the image to a numpy array using img_to_array() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d781c30a76043507",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_img_array = image.img_to_array(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc71fecc292984a",
   "metadata": {},
   "source": [
    "### Expand the dimensions of the array to match the input shape of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc542738f2f14cf1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_img_array = np.expand_dims(test_img_array, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12abdf8c1bd5d814",
   "metadata": {},
   "source": [
    "### Preprocess the image, in this it normalizes the pixel values in the `test_img_array` NumPy array by dividing each element by 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba524e9a8698b64c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_img_array = test_img_array / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff60afeeecdfabcb",
   "metadata": {},
   "source": [
    "### Make predictions using the trained model. The model takes the preprocessed image data `(test_img_array)` as input and produces predictions or output values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e010984497eb0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_img_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a4b772cbde2b5",
   "metadata": {},
   "source": [
    "### Get the class label with the highest probability using function argmax() to determine the index of the highest value in the prediction array, which identify the class or category with the highest predicted probability from the model's output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28818557a804c59f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "predicted_class = np.argmax(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a7ec958f7aeeda",
   "metadata": {},
   "source": [
    "### .class_indices retrieves the dictionary of class indices from the train_generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4df90177e88cded",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class_indices = train_generator.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1f74020310d64",
   "metadata": {},
   "source": [
    "### Get the class label corresponding to the predicted class. In this we iterate through the class_indices dictionary, searching for the key-value pair where the value matches the `predicted_class`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be55a8132fb6dbd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "predicted_label = [k for k, v in class_indices.items() if v == predicted_class][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06181bf384dc3f",
   "metadata": {},
   "source": [
    "### Print the predicted label and the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23a6ffa0a69dd01",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(f'The predicted label is: {predicted_label}')\n",
    "plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8edd72f-e7b0-47a9-b84d-6c244c247080",
   "metadata": {},
   "source": [
    "## **Graph Plotting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdfe51c77be417b",
   "metadata": {},
   "source": [
    "### Plot training and validation accuracy over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4170c2294fdc8844",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6cd5d2d5cc6660",
   "metadata": {},
   "source": [
    "### Plot the training and validation loss over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36d3873652ce73f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classifier:Python",
   "language": "python",
   "name": "conda-env-classifier-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
